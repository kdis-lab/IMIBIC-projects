perms(c(1,2,3))
?which
install.packages("grConvert")
install.packages("grConvert", repos="http://R-Forge.R-project.org")
install.packages("grImport", repos="http://R-Forge.R-project.org")
install.packages("grImport2", repos="http://R-Forge.R-project.org")
install.packages("devtools")
library(devtools)
install_github("sjp/grConvert")
install_github("sjp/grConvert")
library(grConverter)
library(grConvert)
setwd("D://OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
dataset <- read.csv("4horas.csv",na.strings = "")
View(dataset)
View(dataset)
dataset[,"RNU1"] <- NULL
View(dataset)
write.table(dataset, file= paste(fileT, "4horas-withoutRNU1.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
write.table(dataset, file= paste("4horas-withoutRNU1.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
cerohora <- read.csv("0horas.csv")
cuatrohora <- read.csv("4horas-withoutRNU1.csv")
id0horas <- read.csv("id0horas.csv")
id4horas <- read.csv("id4horas.csv")
rownames(cerohora) <- id0horas[,1]
rownames(cuatrohora) <- id4horas[,1]
colnames(cerohora) <- paste("0h", colnames(cerohora), sep = "")
colnames(cuatrohora) <- paste("4h", colnames(cuatrohora), sep = "")
#Detecting the ids that exist in both datasets
intersections <- intersect(id0horas[,1], id4horas[,1])
cerohora <- cerohora[match(intersections, rownames(cerohora)), 1:(ncol(cerohora)-1)]
cuatrohora <- cuatrohora[match(intersections, rownames(cuatrohora)), 1:ncol(cuatrohora)]
newData <- cbind(cerohora, cuatrohora)
write.table(newData, file = "0-4horas-withoutRNu1.csv", quote = FALSE,
sep=",", row.names = FALSE, col.names = TRUE)
setwd("D://OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
dataset <- read.csv("0-4horas-withoutRNu1.csv",na.strings = "")
View(dataset)
f
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
cerohora <- read.csv("0horas.csv")
cuatrohora <- read.csv("4horas-withoutRNU1.csv")
id0horas <- read.csv("id0horas.csv")
id4horas <- read.csv("id4horas.csv")
rownames(cerohora) <- id0horas[,1]
rownames(cuatrohora) <- id4horas[,1]
colnames(cerohora) <- paste("0h", colnames(cerohora), sep = "")
colnames(cuatrohora) <- paste("4h", colnames(cuatrohora), sep = "")
#Detecting the ids that exist in both datasets
intersections <- intersect(id0horas[,1], id4horas[,1])
cerohora <- cerohora[match(intersections, rownames(cerohora)), 1:(ncol(cerohora)-1)]
cuatrohora <- cuatrohora[match(intersections, rownames(cuatrohora)), 1:ncol(cuatrohora)]
newData <- cbind(cerohora, cuatrohora)
write.table(newData, file = "0-4horas-withoutRNu1.csv", quote = FALSE,
sep=",", row.names = FALSE, col.names = TRUE, na = "")
setwd("D://OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
dataset <- read.csv("0-4horas-withoutRNu1.csv",na.strings = "")
View(dataset)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "0-4horas-withoutRNu1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "0-4horas-withoutRNu1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
threshold <- 0.3
cnames <- colnames(dataset)[1:16]
for(index in cnames){
sumaNA<-sum(is.na(dataset[,index]))
indexesCeros <- dataset[,index]==0 & !is.na(dataset[,index])
sumacero <- sum(indexesCeros)
to <- sumaNA+sumacero
#Remove the factor if the number of NA and ceros is greater than the threshold
if(to/nrow(dataset) >= threshold){
print("Removing...")
print(paste(index,": NA (", sumaNA, ") ", "Ceros (", sumacero , ") percent:", to/nrow(dataset)))
dataset[,index] <-  NULL
}
else{
#removing Os
dataset[indexesCeros, index] <- NA
}
}
methods <- c("zv","nzv","YeoJohnson","center","scale","bagImpute")
preProcValues <- preProcess(dataset, method = methods)
datasetTransformed <- predict(preProcValues, dataset)
#detecting inconsistent or dupplicated examples
datasetTransformed <- datasetTransformed[!duplicated(datasetTransformed[,1:(ncol(datasetTransformed)-1)]),]
write.table(datasetTransformed, file= paste(fileT, "-preproc.csv",sep = ""),
quote = FALSE, sep="," , row.names = TRUE, col.names = TRUE, na = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "0-4horas-withoutRNu1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
threshold <- 0.3
cnames <- colnames(dataset)[1:16]
for(index in cnames){
sumaNA<-sum(is.na(dataset[,index]))
indexesCeros <- dataset[,index]==0 & !is.na(dataset[,index])
sumacero <- sum(indexesCeros)
to <- sumaNA+sumacero
#Remove the factor if the number of NA and ceros is greater than the threshold
if(to/nrow(dataset) >= threshold){
print("Removing...")
print(paste(index,": NA (", sumaNA, ") ", "Ceros (", sumacero , ") percent:", to/nrow(dataset)))
dataset[,index] <-  NULL
}
else{
#removing Os
dataset[indexesCeros, index] <- NA
}
}
methods <- c("zv","nzv","YeoJohnson","center","scale","bagImpute")
preProcValues <- preProcess(dataset, method = methods)
datasetTransformed <- predict(preProcValues, dataset)
#detecting inconsistent or dupplicated examples
datasetTransformed <- datasetTransformed[!duplicated(datasetTransformed[,1:(ncol(datasetTransformed)-1)]),]
write.table(datasetTransformed, file= paste(fileT, "-preproc.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "0-4horas-withoutRNu1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
threshold <- 0.3
cnames <- colnames(dataset)[1:16]
for(index in cnames){
sumaNA<-sum(is.na(dataset[,index]))
indexesCeros <- dataset[,index]==0 & !is.na(dataset[,index])
sumacero <- sum(indexesCeros)
to <- sumaNA+sumacero
#Remove the factor if the number of NA and ceros is greater than the threshold
if(to/nrow(dataset) >= threshold){
print("Removing...")
print(paste(index,": NA (", sumaNA, ") ", "Ceros (", sumacero , ") percent:", to/nrow(dataset)))
dataset[,index] <-  NULL
}
else{
#removing Os
dataset[indexesCeros, index] <- NA
}
}
methods <- c("zv","nzv","YeoJohnson","center","scale","bagImpute")
preProcValues <- preProcess(dataset, method = methods)
datasetTransformed <- predict(preProcValues, dataset)
#detecting inconsistent or dupplicated examples
datasetTransformed <- datasetTransformed[!duplicated(datasetTransformed[,1:(ncol(datasetTransformed)-1)]),]
write.table(datasetTransformed, file= paste(fileT, "-preproc.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "4horas-withoutRNU1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
threshold <- 0.3
cnames <- colnames(dataset)[1:16]
for(index in cnames){
sumaNA<-sum(is.na(dataset[,index]))
indexesCeros <- dataset[,index]==0 & !is.na(dataset[,index])
sumacero <- sum(indexesCeros)
to <- sumaNA+sumacero
#Remove the factor if the number of NA and ceros is greater than the threshold
if(to/nrow(dataset) >= threshold){
print("Removing...")
print(paste(index,": NA (", sumaNA, ") ", "Ceros (", sumacero , ") percent:", to/nrow(dataset)))
dataset[,index] <-  NULL
}
else{
#removing Os
dataset[indexesCeros, index] <- NA
}
}
methods <- c("zv","nzv","YeoJohnson","center","scale","bagImpute")
preProcValues <- preProcess(dataset, method = methods)
datasetTransformed <- predict(preProcValues, dataset)
#detecting inconsistent or dupplicated examples
datasetTransformed <- datasetTransformed[!duplicated(datasetTransformed[,1:(ncol(datasetTransformed)-1)]),]
write.table(datasetTransformed, file= paste(fileT, "-preproc.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "4horas-withoutRNU1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
library(caret)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
fileT <- "4horas-withoutRNU1"
dataset <- read.csv(paste(fileT,".csv",sep = ""), na.strings = "")
threshold <- 0.3
cnames <- colnames(dataset)[1:9]
for(index in cnames){
sumaNA<-sum(is.na(dataset[,index]))
indexesCeros <- dataset[,index]==0 & !is.na(dataset[,index])
sumacero <- sum(indexesCeros)
to <- sumaNA+sumacero
#Remove the factor if the number of NA and ceros is greater than the threshold
if(to/nrow(dataset) >= threshold){
print("Removing...")
print(paste(index,": NA (", sumaNA, ") ", "Ceros (", sumacero , ") percent:", to/nrow(dataset)))
dataset[,index] <-  NULL
}
else{
#removing Os
dataset[indexesCeros, index] <- NA
}
}
methods <- c("zv","nzv","YeoJohnson","center","scale","bagImpute")
preProcValues <- preProcess(dataset, method = methods)
datasetTransformed <- predict(preProcValues, dataset)
#detecting inconsistent or dupplicated examples
datasetTransformed <- datasetTransformed[!duplicated(datasetTransformed[,1:(ncol(datasetTransformed)-1)]),]
write.table(datasetTransformed, file= paste(fileT, "-preproc.csv",sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
library(FSelector)
library(caret)
library(bclust)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/diabetes/v2/")
output <- "results/4horas-withoutRNU1/"
fileName <- "4horas-withoutRNU1"
mydata <- read.csv(paste(fileName,".csv",sep = ""))
# To create a stratified repeated k-fold cross validation. For doing a LOOCV it is needed to set the parameter
# k equal to the number of samples in the dataset
#For replicability
set.seed(123)
folds <- 10
ntimes <- 3
multiIndexes <- createMultiFolds(y = mydata[, ncol(mydata)], k = folds, times = ntimes)
weigthsAverage <- NULL
methodNames <- c("information.gain", "gain.ratio", "symmetrical.uncertainty", "cfs", "consistency", "relief",
"chi.squared")
classColumnName <- colnames(mydata)[ncol(mydata)]
colNames <- colnames(mydata)[-ncol(mydata)]
nameFormula <- as.formula(paste(classColumnName," ~ .", sep = ""))
times <- 0
#We take the rank values as weights
for(indexes in multiIndexes){
subdata <- mydata[indexes,]
numberSamples <- nrow(subdata)
for(method in methodNames){
f <- match.fun(method)
if(method == "relief"){
weigthsReliefF <- NULL
#Repeat the Relief algorithm for various k
for(k in 5:10){
weigths <- f(nameFormula, subdata, neighbours.count = k, sample.size = numberSamples)
weigths$attr_importance <- rank(weigths$attr_importance)
if(is.null(weigthsReliefF)){
weigthsReliefF <- weigths
}
else{
weigthsReliefF <- weigthsReliefF + weigths
}
times <- times +1
}
weigths <- weigthsReliefF
rm(weigthsReliefF)
}
if(method == "cfs"|| method == "consistency"){
subset <- f(nameFormula, subdata)
ind <- match(subset,colNames)
# Only the atts that belong to the subset are set to 1, otherwise are set to 0.
weigths <- data.frame(attr_importance= rep(0, length(colNames)))
rownames(weigths) <- colNames
weigths[ind, "attr_importance"] <- 1
weigths$attr_importance <- rank(weigths$attr_importance)
times <- times +1
}
if(method == "information.gain" || method == "gain.ratio" || method == "symmetrical.uncertainty" || method == "chi.squared"){
weigths <- f(nameFormula, subdata)
weigths$attr_importance <- rank(weigths$attr_importance)
times <- times +1
}
if(is.null(weigthsAverage)){
weigthsAverage <- weigths
}
else{
weigthsAverage <- weigthsAverage + weigths
}
}
}
library(FactoMineR)
library(factoextra)
library(FactoInvestigate)
library(Factoshiny)
library(missMDA)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/enfermedades-autoinmunes/v3/")
dataset <- read.csv("Mono-preproc.csv", na.strings = "")
hasMixedData <- TRUE
hasMissingData <- FALSE
# For handling missing data
################################
if(hasMissingData){
#missMDA is based in a iterative regularized PCA use to estimate missing data. It is a good alternative to other statistical
#techniques for working with missing data
if(!hasMixedData){
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpPCA(dataset[,1:20], ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100, scale = TRUE)
#impute the table
comp <- imputePCA(dataset[,1:20], ncp = nb$ncp, scale = TRUE)
} else{
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpFAMD(dataset, ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100)
#impute the table
comp <- imputeFAMD(dataset, ncp = nb$ncp)
}
}
res <- PCA(#comp$completeObs,
dataset,
scale.unit = TRUE,
quanti.sup = 41,
quali.sup = c(42:50),
graph = FALSE)
par(mar = c(4.1, 4.1, 1.1, 2.1))
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes")
library(FactoMineR)
library(factoextra)
library(FactoInvestigate)
library(Factoshiny)
library(missMDA)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/enfermedades-autoinmunes/v3/")
dataset <- read.csv("Linfo-preproc.csv", na.strings = "")
hasMixedData <- TRUE
hasMissingData <- FALSE
# For handling missing data
################################
if(hasMissingData){
#missMDA is based in a iterative regularized PCA use to estimate missing data. It is a good alternative to other statistical
#techniques for working with missing data
if(!hasMixedData){
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpPCA(dataset[,1:20], ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100, scale = TRUE)
#impute the table
comp <- imputePCA(dataset[,1:20], ncp = nb$ncp, scale = TRUE)
} else{
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpFAMD(dataset, ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100)
#impute the table
comp <- imputeFAMD(dataset, ncp = nb$ncp)
}
}
View(dataset)
res <- PCA(#comp$completeObs,
dataset,
scale.unit = TRUE,
quanti.sup = 35,
quali.sup = c(36:44),
graph = FALSE)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes")
par(mar = c(4.1, 4.1, 1.1, 2.1))
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes")
sample = sample(rownames(res$call$X), length(rownames(res$call$X)))
res$call$X = res$call$X[sample,]
res$ind$coord = res$ind$coord[sample[!sample %in% rownames(res$ind.sup$coord)],]
res$ind.sup$coord = res$ind.sup$coord[sample[sample %in% rownames(res$ind.sup$coord)],]
drawn <-
c("2", "14", "27", "16", "35", "32", "28", "31", "33", "34",
"12", "15", "29", "4", "6", "17", "30", "11")
View(dataset)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 39)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 40)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 39)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 39, select = drawn)
drawn <-
c("30", "4", "24", "25", "10", "2", "18", "29", "9", "22", "32",
"14", "27", "1", "13", "7", "12", "15")
par(mar = c(4.1, 4.1, 1.1, 2.1))
plot.PCA(res, axes = 3:4, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 42, select = drawn)
HCPCshiny(res)
library(FactoMineR)
library(factoextra)
library(FactoInvestigate)
library(Factoshiny)
library(missMDA)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/enfermedades-autoinmunes/v3/")
dataset <- read.csv("Neutro-preproc.csv", na.strings = "")
hasMixedData <- TRUE
hasMissingData <- FALSE
# For handling missing data
################################
if(hasMissingData){
#missMDA is based in a iterative regularized PCA use to estimate missing data. It is a good alternative to other statistical
#techniques for working with missing data
if(!hasMixedData){
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpPCA(dataset[,1:20], ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100, scale = TRUE)
#impute the table
comp <- imputePCA(dataset[,1:20], ncp = nb$ncp, scale = TRUE)
} else{
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpFAMD(dataset, ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100)
#impute the table
comp <- imputeFAMD(dataset, ncp = nb$ncp)
}
}
res <- PCA(#comp$completeObs,
dataset,
scale.unit = TRUE,
quanti.sup = 35,
quali.sup = c(36:44),
graph = FALSE)
library(FactoMineR)
library(factoextra)
library(FactoInvestigate)
library(Factoshiny)
library(missMDA)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/enfermedades-autoinmunes/v3/")
dataset <- read.csv("Neutro-preproc.csv", na.strings = "")
hasMixedData <- TRUE
hasMissingData <- FALSE
# For handling missing data
################################
if(hasMissingData){
#missMDA is based in a iterative regularized PCA use to estimate missing data. It is a good alternative to other statistical
#techniques for working with missing data
if(!hasMixedData){
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpPCA(dataset[,1:20], ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100, scale = TRUE)
#impute the table
comp <- imputePCA(dataset[,1:20], ncp = nb$ncp, scale = TRUE)
} else{
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpFAMD(dataset, ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100)
#impute the table
comp <- imputeFAMD(dataset, ncp = nb$ncp)
}
}
View(dataset)
res <- PCA(#comp$completeObs,
dataset,
scale.unit = TRUE,
quanti.sup = 30,
quali.sup = c(31:39),
graph = FALSE)
library(FactoMineR)
library(factoextra)
library(FactoInvestigate)
library(Factoshiny)
library(missMDA)
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/enfermedades-autoinmunes/v3/")
dataset <- read.csv("Neutro-preproc.csv", na.strings = "")
hasMixedData <- TRUE
hasMissingData <- FALSE
# For handling missing data
################################
if(hasMissingData){
#missMDA is based in a iterative regularized PCA use to estimate missing data. It is a good alternative to other statistical
#techniques for working with missing data
if(!hasMixedData){
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpPCA(dataset[,1:20], ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100, scale = TRUE)
#impute the table
comp <- imputePCA(dataset[,1:20], ncp = nb$ncp, scale = TRUE)
} else{
# This function finds the optimal number of components (dimensions) to use when inputing missing data
nb <- estim_ncpFAMD(dataset, ncp.min=0, ncp.max=5, method.cv="Kfold", nbsim=100)
#impute the table
comp <- imputeFAMD(dataset, ncp = nb$ncp)
}
}
res <- PCA(#comp$completeObs,
dataset,
scale.unit = TRUE,
quanti.sup = 30,
quali.sup = c(31:39),
graph = FALSE)
par(mar = c(4.1, 4.1, 1.1, 2.1))
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes")
drawn <-
c("9", "26", "8", "7", "13", "1", "16", "27", "18", "29", "32",
"25", "5", "11", "20", "15", "10", "24")
par(mar = c(4.1, 4.1, 1.1, 2.1))
View(dataset)
View(dataset)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 32)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 32)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 32)
par(mar = c(4.1, 4.1, 1.1, 2.1))
drawn <-
c("9", "26", "8", "7", "13", "1", "16", "27", "18", "29", "32",
"25", "5", "11", "20", "15", "10", "24")
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 32, select = drawn)
drawn <-
c("28", "18", "1", "30", "33", "26", "20", "15", "25", "19",
"23", "21", "5", "17", "8", "24", "4", "12")
par(mar = c(4.1, 4.1, 1.1, 2.1))
View(dataset)
plot.PCA(res, axes = 3:4, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 35, select = drawn)
HCPCshiny(res)
res.hcpc<-HCPC(res)
res.hcpc$desc.var
HCPCshiny(res)
HCPC(res)
HCPC(res)
HCPC(res)
res.hcpc$desc.var
res.hcpc <- HCPC(res)
res.hcpc$desc.var
View(dataset)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 33
)
View(dataset)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 35)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 34)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 36)
res.hcpc$desc.var
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 33)
res.hcpc$desc.var
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 34)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 33)
plot.PCA(res, axes = 1:2, choix = 'ind', invisible = 'quali', title = '', cex = 0.5, autoLab = "yes", habillage = 35)
