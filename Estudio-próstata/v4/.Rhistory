xlab=secondCol, ylab=firstCol)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=5, units="in", res=200)
dev.off()
}
if(class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="factor"){
# With a large sample size, even a small degree of association can show a significant Chi squared.
# Instead, there are a variety of statistical measures of strength of association for contingency tables- similar
# in spirit to r or r-squared for continuous variables.
#ntest <- chisq.test(mydata[,firstCol],mydata[,secondCol])
# Cramer's V is the most popular of the chi-square-based measures of nominal association
#because it gives good norming from 0 to 1 regardless of table size
cramer<-assocstats(table(mydata[,firstCol],mydata[,secondCol]))$cramer
#print(paste(firstCol,"-",secondCol," ",ntest$statistic," ",ntest$p.value, sep = ""))
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Factorvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = cramer))
# Mosaic plots provide an ideal method both for visualizing contingency tables and for visualizing the
# fit- or more importantly- lack of fit of a loglinear model.
# For a two-way table, mosaic() fits a model of independence
#For n-way tables, mosaic() can fit any loglinear model
#The colors represent the level of the residual for that cell / combination of levels.
#The legend is presented at the plot's right. More specifically, blue means there are
#more observations in that cell than would be expected under the null model
#(independence). Red means there are fewer observations than would have been
#expected. You can read this as showing you which cells are contributing to the
#significance of the chi-squared test result.
#Si los residuos no son largos, yet the
#association between the two variables is highly significant.
formulaT <- as.formula(paste("~", firstCol,"+", secondCol,sep = ""))
mosaic(formulaT, data= mydata, gp = shading_Friendly, split_vertical = TRUE, legend =TRUE, labeling=labeling_border(
rot_labels = c(90, 0, 90, 0),
just_labels=c("left","left","right","right"),
tl_varnames = FALSE,
gp_labels = gpar(fontsize = 9)))
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=10, height=10, units="in", res=200)
dev.off()
}
#NumericvFactor
if((class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="integer") || (class(mydata[,firstCol])=="integer" && class(mydata[,secondCol])=="factor")){
theFactor <- ifelse(class(mydata[,firstCol])=="factor", firstCol, secondCol)
theNumeric <- ifelse(class(mydata[,firstCol])=="integer", firstCol, secondCol)
#The strength of correlation between a categorical variable (dichotomous) and an interval/ratio variable can be computed using point biserial correlation. If your categorical variable has more than two measures, you try eta correlation.
#How can I calculate the correlation between a categorical independent variable and a continuous dependent variable?. Available from: https://www.researchgate.net/post/How_can_I_calculate_the_correlation_between_a_categorical_independent_variable_and_a_continuous_dependent_variable
formulaT <- as.formula(paste(theNumeric,"~",theFactor,sep = ""))
#Eta squared is a measured of effect size, it is analougous to r-squared. It represents the proportion of variance in Y explained by X. It can detect non-linear correlations
# It ranges between 0..1. A rule of thumb (Cohen):
# 0.02 -small
# 0.13 -medium
# 0.26 -large
anovaTest <- aov( formula = formulaT, data= mydata)
corre <- etaSquared(anovaTest)[1][1]
#Storing the AUC
#print(formulaT)
#corre <-lrm(formulaT, data = mydata)$stats["C"]
formulaT <- as.formula(paste(theFactor,"~",theNumeric,sep = ""))
cdplot(formulaT, data = mydata)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
}
}
}
#Displaying a correlogram for the numeric variables
corrgram(mydata, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie)
dev.copy(png,paste(output, "Correlogram between numeric vars.png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
# Sort by Type and statistic
newData <- newData[order(newData$Type,-newData$Statistic), ]
write.table(newData, file = paste(output,"Correlations.csv",sep = ""),
quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
chisq.test(mydata$Hbpressure,mydata$PerineuralInv)
View(mydata)
class(mydata$BMI)
library(car)
library(caret)
library(polycor)
library(vcd)
library(vcdExtra)
library(corrgram)
library(rms)
library(lsr)
#For more details of measures of associations consult
#http://dwoll.de/rexrepos/posts/associationOrder.html
#https://www.andrews.edu/~calkins/math/edrm611/edrm13.htm
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/cancer/v4/")
mydata <- read.csv("factores-clinicos-withoutOutliersv2.csv", sep = ",", na.strings = "")
output <- "results/Correlations/"
newData <- data.frame("Type" = character(0),
"Comparison" = character(0),
"Statistic" = numeric(0))
#Computes a heterogenous correlation matrix, consisting of Pearson product-moment correlations
#between numeric variables, polyserial correlations between numeric and ordinal variables, and
#polychoric correlations between ordinal variables.
#results <-hetcor(mydata)
# very important
#https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php
# Test the existence of relathionships between variables
cNames <- colnames(mydata)
ncolumns <- ncol(mydata)
for(i in 1: (ncolumns-1)){
firstCol <- cNames[i]
for(j in (i+1):ncolumns) {
secondCol <- cNames[j]
#scatter plots
if((class(mydata[,firstCol])=="integer" || class(mydata[,firstCol])=="numeric")
&& (class(mydata[,secondCol])=="integer" || class(mydata[,secondCol])=="numeric")){
# Using parametric assumptions (Pearson, dividing the coefficient by its standard error, giving a value that follow
# a t-distribution) or when data violate parametric assumptions using Spearman rank coefficient.
corre <- cor(mydata[,firstCol], mydata[,secondCol], method="pearson", use="na.or.complete")
#the following guidelines have been proposed:
#Coefficient, r
#Strength of Association 	Positive 	Negative
#Small                	.1 to .3 	-0.1 to -0.3
#Medium 	              .3 to .5 	-0.3 to -0.5
#Large 	                .5 to 1.0 	-0.5 to -1.0
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Numeric"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
formulaT <- as.formula(paste(firstCol," ~ ", secondCol, sep =""))
scatterplot(formulaT, data=mydata,
xlab=secondCol, ylab=firstCol)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=5, units="in", res=200)
dev.off()
}
if(class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="factor"){
# With a large sample size, even a small degree of association can show a significant Chi squared.
# Instead, there are a variety of statistical measures of strength of association for contingency tables- similar
# in spirit to r or r-squared for continuous variables.
#ntest <- chisq.test(mydata[,firstCol],mydata[,secondCol])
# Cramer's V is the most popular of the chi-square-based measures of nominal association
#because it gives good norming from 0 to 1 regardless of table size
#It generally has a maximum value of 1 when there is a very strong relationship
#between two variables
#In practice, you may find that a Cramer's V of .10 provides a good minimum threshold for suggesting there is a substantive relationship between two variables.
cramer<-assocstats(table(mydata[,firstCol],mydata[,secondCol]))$cramer
#print(paste(firstCol,"-",secondCol," ",ntest$statistic," ",ntest$p.value, sep = ""))
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Factorvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = cramer))
# Mosaic plots provide an ideal method both for visualizing contingency tables and for visualizing the
# fit- or more importantly- lack of fit of a loglinear model.
# For a two-way table, mosaic() fits a model of independence
#For n-way tables, mosaic() can fit any loglinear model
#The colors represent the level of the residual for that cell / combination of levels.
#The legend is presented at the plot's right. More specifically, blue means there are
#more observations in that cell than would be expected under the null model
#(independence). Red means there are fewer observations than would have been
#expected. You can read this as showing you which cells are contributing to the
#significance of the chi-squared test result.
#Si los residuos no son largos, yet the
#association between the two variables is highly significant.
formulaT <- as.formula(paste("~", firstCol,"+", secondCol,sep = ""))
mosaic(formulaT, data= mydata, gp = shading_Friendly, split_vertical = TRUE, legend =TRUE, labeling=labeling_border(
rot_labels = c(90, 0, 90, 0),
just_labels=c("left","left","right","right"),
tl_varnames = FALSE,
gp_labels = gpar(fontsize = 9)))
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=10, height=10, units="in", res=200)
dev.off()
}
#NumericvFactor
if((class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="integer")
|| (class(mydata[,firstCol])=="integer" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="numeric" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="numeric"))
{
theFactor <- ifelse(class(mydata[,firstCol])=="factor", firstCol, secondCol)
theNumeric <- ifelse(class(mydata[,firstCol])=="integer", firstCol, secondCol)
#The strength of correlation between a categorical variable (dichotomous) and an interval/ratio variable can be computed using point biserial correlation. If your categorical variable has more than two measures, you try eta correlation.
#How can I calculate the correlation between a categorical independent variable and a continuous dependent variable?. Available from: https://www.researchgate.net/post/How_can_I_calculate_the_correlation_between_a_categorical_independent_variable_and_a_continuous_dependent_variable
formulaT <- as.formula(paste(theNumeric,"~",theFactor,sep = ""))
#Eta squared is a measured of effect size, it is analougous to r-squared. It represents the proportion of variance in Y explained by X. It can detect non-linear correlations
# It ranges between 0..1. A rule of thumb (Cohen):
# 0.02 -small
# 0.13 -medium
# 0.26 -large
anovaTest <- aov( formula = formulaT, data= mydata)
corre <- etaSquared(anovaTest)[1][1]
#Storing the AUC
#print(formulaT)
#corre <-lrm(formulaT, data = mydata)$stats["C"]
formulaT <- as.formula(paste(theFactor,"~",theNumeric,sep = ""))
cdplot(formulaT, data = mydata)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
}
}
}
#Displaying a correlogram for the numeric variables
corrgram(mydata, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie)
dev.copy(png,paste(output, "Correlogram between numeric vars.png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
# Sort by Type and statistic
newData <- newData[order(newData$Type,-newData$Statistic), ]
write.table(newData, file = paste(output,"Correlations.csv",sep = ""),
quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
?assocstats
library(car)
library(caret)
library(polycor)
library(vcd)
library(vcdExtra)
library(corrgram)
library(rms)
library(lsr)
#For more details of measures of associations consult
#http://dwoll.de/rexrepos/posts/associationOrder.html
#https://www.andrews.edu/~calkins/math/edrm611/edrm13.htm
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/cancer/v4/")
mydata <- read.csv("factores-clinicos-withoutOutliersv2.csv", sep = ",", na.strings = "")
output <- "results/Correlations/"
newData <- data.frame("Type" = character(0),
"Comparison" = character(0),
"Statistic" = numeric(0))
#Computes a heterogenous correlation matrix, consisting of Pearson product-moment correlations
#between numeric variables, polyserial correlations between numeric and ordinal variables, and
#polychoric correlations between ordinal variables.
#results <-hetcor(mydata)
# very important
#https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php
# Test the existence of relathionships between variables
cNames <- colnames(mydata)
ncolumns <- ncol(mydata)
for(i in 1: (ncolumns-1)){
firstCol <- cNames[i]
for(j in (i+1):ncolumns) {
secondCol <- cNames[j]
#scatter plots
if((class(mydata[,firstCol])=="integer" || class(mydata[,firstCol])=="numeric")
&& (class(mydata[,secondCol])=="integer" || class(mydata[,secondCol])=="numeric")){
# Using parametric assumptions (Pearson, dividing the coefficient by its standard error, giving a value that follow
# a t-distribution) or when data violate parametric assumptions using Spearman rank coefficient.
corre <- cor(mydata[,firstCol], mydata[,secondCol], method="pearson", use="na.or.complete")
#the following guidelines have been proposed:
#Coefficient, r
#Strength of Association 	Positive 	Negative
#Small                	.1 to .3 	-0.1 to -0.3
#Medium 	              .3 to .5 	-0.3 to -0.5
#Large 	                .5 to 1.0 	-0.5 to -1.0
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Numeric"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
formulaT <- as.formula(paste(firstCol," ~ ", secondCol, sep =""))
scatterplot(formulaT, data=mydata,
xlab=secondCol, ylab=firstCol)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=5, units="in", res=200)
dev.off()
}
if(class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="factor"){
# With a large sample size, even a small degree of association can show a significant Chi squared.
# Instead, there are a variety of statistical measures of strength of association for contingency tables- similar
# in spirit to r or r-squared for continuous variables.
#ntest <- chisq.test(mydata[,firstCol],mydata[,secondCol])
# Cramer's V is the most popular of the chi-square-based measures of nominal association
#because it gives good norming from 0 to 1 regardless of table size
#It generally has a maximum value of 1 when there is a very strong relationship
#between two variables
#In practice, you may find that a Cramer's V of .10 provides a good minimum threshold for suggesting there is a substantive relationship between two variables.
cramer<-assocstats(table(mydata[,firstCol],mydata[,secondCol]))$cramer
#print(paste(firstCol,"-",secondCol," ",ntest$statistic," ",ntest$p.value, sep = ""))
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Factorvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = cramer))
# Mosaic plots provide an ideal method both for visualizing contingency tables and for visualizing the
# fit- or more importantly- lack of fit of a loglinear model.
# For a two-way table, mosaic() fits a model of independence
#For n-way tables, mosaic() can fit any loglinear model
#The colors represent the level of the residual for that cell / combination of levels.
#The legend is presented at the plot's right. More specifically, blue means there are
#more observations in that cell than would be expected under the null model
#(independence). Red means there are fewer observations than would have been
#expected. You can read this as showing you which cells are contributing to the
#significance of the chi-squared test result.
#Si los residuos no son largos, yet the
#association between the two variables is highly significant.
formulaT <- as.formula(paste("~", firstCol,"+", secondCol,sep = ""))
mosaic(formulaT, data= mydata, gp = shading_Friendly, split_vertical = TRUE, legend =TRUE, labeling=labeling_border(
rot_labels = c(90, 0, 90, 0),
just_labels=c("left","left","right","right"),
tl_varnames = FALSE,
gp_labels = gpar(fontsize = 9)))
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=10, height=10, units="in", res=200)
dev.off()
}
#NumericvFactor
if((class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="integer")
|| (class(mydata[,firstCol])=="integer" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="numeric" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="numeric"))
{
theFactor <- ifelse(class(mydata[,firstCol])=="factor", firstCol, secondCol)
theNumeric <- ifelse(class(mydata[,firstCol])=="integer", firstCol, secondCol)
#The strength of correlation between a categorical variable (dichotomous) and an interval/ratio variable can be computed using point biserial correlation. If your categorical variable has more than two measures, you try eta correlation.
#How can I calculate the correlation between a categorical independent variable and a continuous dependent variable?. Available from: https://www.researchgate.net/post/How_can_I_calculate_the_correlation_between_a_categorical_independent_variable_and_a_continuous_dependent_variable
formulaT <- as.formula(paste(theNumeric,"~",theFactor,sep = ""))
#Eta squared is a measured of effect size, it is analougous to r-squared. It represents the proportion of variance in Y explained by X. It can detect non-linear correlations
# It ranges between 0..1. A rule of thumb (Cohen):
# 0.02 -small
# 0.13 -medium
# 0.26 -large
anovaTest <- aov( formula = formulaT, data= mydata)
corre <- etaSquared(anovaTest)[1][1]
#Storing the AUC
#print(formulaT)
#corre <-lrm(formulaT, data = mydata)$stats["C"]
formulaT <- as.formula(paste(theFactor,"~",theNumeric,sep = ""))
cdplot(formulaT, data = mydata)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
}
}
}
#Displaying a correlogram for the numeric variables
corrgram(mydata, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie)
dev.copy(png,paste(output, "Correlogram between numeric vars.png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
# Sort by Type and statistic
newData <- newData[order(newData$Type,-newData$Statistic), ]
write.table(newData, file = paste(output,"Correlations.csv",sep = ""),
quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
firstCol
secondCol
library(car)
library(caret)
library(polycor)
library(vcd)
library(vcdExtra)
library(corrgram)
library(rms)
library(lsr)
#For more details of measures of associations consult
#http://dwoll.de/rexrepos/posts/associationOrder.html
#https://www.andrews.edu/~calkins/math/edrm611/edrm13.htm
setwd("D:/OneDrive - Universidad de Córdoba/workspace/IMIBIC/datasets/cancer/v4/")
mydata <- read.csv("factores-clinicos-withoutOutliersv2.csv", sep = ",", na.strings = "")
output <- "results/Correlations/"
newData <- data.frame("Type" = character(0),
"Comparison" = character(0),
"Statistic" = numeric(0))
#Computes a heterogenous correlation matrix, consisting of Pearson product-moment correlations
#between numeric variables, polyserial correlations between numeric and ordinal variables, and
#polychoric correlations between ordinal variables.
#results <-hetcor(mydata)
# very important
#https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php
# Test the existence of relathionships between variables
cNames <- colnames(mydata)
ncolumns <- ncol(mydata)
for(i in 1: (ncolumns-1)){
firstCol <- cNames[i]
for(j in (i+1):ncolumns) {
secondCol <- cNames[j]
#scatter plots
if((class(mydata[,firstCol])=="integer" || class(mydata[,firstCol])=="numeric")
&& (class(mydata[,secondCol])=="integer" || class(mydata[,secondCol])=="numeric")){
# Using parametric assumptions (Pearson, dividing the coefficient by its standard error, giving a value that follow
# a t-distribution) or when data violate parametric assumptions using Spearman rank coefficient.
corre <- cor(mydata[,firstCol], mydata[,secondCol], method="pearson", use="na.or.complete")
#the following guidelines have been proposed:
#Coefficient, r
#Strength of Association 	Positive 	Negative
#Small                	.1 to .3 	-0.1 to -0.3
#Medium 	              .3 to .5 	-0.3 to -0.5
#Large 	                .5 to 1.0 	-0.5 to -1.0
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Numeric"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
formulaT <- as.formula(paste(firstCol," ~ ", secondCol, sep =""))
scatterplot(formulaT, data=mydata,
xlab=secondCol, ylab=firstCol)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=5, units="in", res=200)
dev.off()
}
if(class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="factor"){
# With a large sample size, even a small degree of association can show a significant Chi squared.
# Instead, there are a variety of statistical measures of strength of association for contingency tables- similar
# in spirit to r or r-squared for continuous variables.
#ntest <- chisq.test(mydata[,firstCol],mydata[,secondCol])
# Cramer's V is the most popular of the chi-square-based measures of nominal association
#because it gives good norming from 0 to 1 regardless of table size
#It generally has a maximum value of 1 when there is a very strong relationship
#between two variables
#In practice, you may find that a Cramer's V of .10 provides a good minimum threshold for suggesting there is a substantive relationship between two variables.
cramer<-assocstats(table(mydata[,firstCol],mydata[,secondCol]))$cramer
#print(paste(firstCol,"-",secondCol," ",ntest$statistic," ",ntest$p.value, sep = ""))
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Factorvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = cramer))
# Mosaic plots provide an ideal method both for visualizing contingency tables and for visualizing the
# fit- or more importantly- lack of fit of a loglinear model.
# For a two-way table, mosaic() fits a model of independence
#For n-way tables, mosaic() can fit any loglinear model
#The colors represent the level of the residual for that cell / combination of levels.
#The legend is presented at the plot's right. More specifically, blue means there are
#more observations in that cell than would be expected under the null model
#(independence). Red means there are fewer observations than would have been
#expected. You can read this as showing you which cells are contributing to the
#significance of the chi-squared test result.
#Si los residuos no son largos, yet the
#association between the two variables is highly significant.
formulaT <- as.formula(paste("~", firstCol,"+", secondCol,sep = ""))
mosaic(formulaT, data= mydata, gp = shading_Friendly, split_vertical = TRUE, legend =TRUE, labeling=labeling_border(
rot_labels = c(90, 0, 90, 0),
just_labels=c("left","left","right","right"),
tl_varnames = FALSE,
gp_labels = gpar(fontsize = 9)))
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=10, height=10, units="in", res=200)
dev.off()
}
#NumericvFactor
if((class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="integer")
|| (class(mydata[,firstCol])=="integer" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="numeric" && class(mydata[,secondCol])=="factor")
|| (class(mydata[,firstCol])=="factor" && class(mydata[,secondCol])=="numeric"))
{
theFactor <- ifelse(class(mydata[,firstCol])=="factor", firstCol, secondCol)
theNumeric <- ifelse(class(mydata[,firstCol])=="integer" || class(mydata[,firstCol])=="numeric", firstCol, secondCol)
#The strength of correlation between a categorical variable (dichotomous) and an interval/ratio variable can be computed using point biserial correlation. If your categorical variable has more than two measures, you try eta correlation.
#How can I calculate the correlation between a categorical independent variable and a continuous dependent variable?. Available from: https://www.researchgate.net/post/How_can_I_calculate_the_correlation_between_a_categorical_independent_variable_and_a_continuous_dependent_variable
formulaT <- as.formula(paste(theNumeric,"~",theFactor,sep = ""))
#Eta squared is a measured of effect size, it is analougous to r-squared. It represents the proportion of variance in Y explained by X. It can detect non-linear correlations
# It ranges between 0..1. A rule of thumb (Cohen):
# 0.02 -small
# 0.13 -medium
# 0.26 -large
anovaTest <- aov( formula = formulaT, data= mydata)
corre <- etaSquared(anovaTest)[1][1]
#Storing the AUC
#print(formulaT)
#corre <-lrm(formulaT, data = mydata)$stats["C"]
formulaT <- as.formula(paste(theFactor,"~",theNumeric,sep = ""))
cdplot(formulaT, data = mydata)
dev.copy(png,paste(output, firstCol,"vs",secondCol,".png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
#register the comparison
newData <-
rbind(
newData,
data.frame(
"Type" = paste("Numericvs.Factor"),
"Comparison" = paste(firstCol,"-",secondCol,sep = ""),
"Statistic" = corre))
}
}
}
#Displaying a correlogram for the numeric variables
corrgram(mydata, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie)
dev.copy(png,paste(output, "Correlogram between numeric vars.png",sep = ""), width=8, height=8, units="in", res=200)
dev.off()
# Sort by Type and statistic
newData <- newData[order(newData$Type,-newData$Statistic), ]
write.table(newData, file = paste(output,"Correlations.csv",sep = ""),
quote = FALSE, sep = ",", row.names = FALSE, col.names = TRUE)
class(mydata$Arexp)
