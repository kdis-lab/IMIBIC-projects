{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_different_timesteps(dataset):\n",
    "    \"\"\"\n",
    "    dataset: is the original dataset with all numeric columns\n",
    "\n",
    "    This function returns a tuple of lenght 4.\n",
    "    encoder_input_data: list of list\n",
    "    decoder_input_data: list of FG values\n",
    "    decoder_target_data: list of FG values to predict\n",
    "    buckets: dictionary with info of samples with the same timesteps. Key is the number of timesteps, \n",
    "    and value is a list with the indexes of the samples which have this number of timesteps\"\"\"\n",
    "\n",
    "    # Represents the encoder input data for each sample\n",
    "    encoder_input_data = []\n",
    "    # Represents the decoder input data for each sample\n",
    "    decoder_input_data= []\n",
    "    # Represents the decoder target data for each sample\n",
    "    decoder_target_data= []\n",
    "\n",
    "    # Each entry of this list represents the FG value to be predicted for each example\n",
    "    y = []\n",
    "\n",
    "    buckects = {}\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    # For each patient\n",
    "    for index, rows in dataset.groupby(\"Id\"):\n",
    "\n",
    "        if rows.shape == 1:\n",
    "            continue\n",
    "\n",
    "        # Select all columns except the column 'Id'\n",
    "        rows = rows.iloc[:, 1:]\n",
    "\n",
    "        num_visits = rows.shape[0]\n",
    "\n",
    "        for visit in range(1, num_visits):\n",
    "\n",
    "            visit_data = []\n",
    "\n",
    "            rows.iloc[:visit, :].apply(\n",
    "                lambda row: visit_data.extend(list(row.values)), axis=1)\n",
    "            \n",
    "            # El decoder recibira el ultimo FG conocido. Es decir el primer FG pasado al decoder,\n",
    "            # es el ultimo valor de FG que se le pasa al encoder (el Ãºltimo FG de visita). El resto de valores de FG no son pasados al encoder.\n",
    "            \n",
    "            FG_values = rows[\"Filtrado_glomerular\"].values[(visit-1):].tolist()\n",
    "\n",
    "            encoder_input_data.append(visit_data)\n",
    "            # El decoder_input_data tendra los valores de FG que se quieren predecir +1, el adicional es el ultimo \n",
    "            #valor conocido de FG\n",
    "            decoder_input_data.append(FG_values[:-1])\n",
    "            # El decoder_target_data tendra los valores de FG que se quieren predecir\n",
    "            decoder_target_data.append(FG_values[1:])\n",
    "\n",
    "            if visit not in buckects:\n",
    "                buckects[visit] = [pos]\n",
    "            else:\n",
    "                buckects[visit].append(pos)\n",
    "\n",
    "            pos += 1\n",
    "            \n",
    "    #decoder_target_data = sequence.pad_sequences(decoder_target_data, dtype=\"float32\", padding='post', value= -1000).tolist()\n",
    "\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data, buckects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(encoder_X, decoder_X, decoder_y, num_features):\n",
    "\n",
    "    i = 0\n",
    "    nsamples = len(encoder_X)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        enc_x = encoder_X[i]  # get the list representing the predictive vars\n",
    "        dec_x = decoder_X[i]\n",
    "        dec_y = decoder_y[i]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        timesteps_encoder = int(len(enc_x) / num_features)\n",
    "        timesteps_decoder = len(dec_x)\n",
    "\n",
    "        enc_x = np.array(enc_x).reshape((1, timesteps_encoder, num_features))\n",
    "        dec_x = np.array(dec_x).reshape((1, timesteps_decoder, 1))\n",
    "        dec_y = np.array(dec_y).reshape((1, timesteps_decoder, 1))\n",
    "        \n",
    "        yield [enc_x, dec_x], dec_y\n",
    "\n",
    "        # Reset the counter\n",
    "        if i == nsamples:\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskValue= -1000\n",
    "\n",
    "def custom_loss(yTrue,yPred):\n",
    "    \n",
    "    #find which values in yTrue (target) are the mask value\n",
    "    isMask = K.equal(yTrue, maskValue) #true for all mask values\n",
    "\n",
    "    #transform to float (0 or 1) and invert\n",
    "    isMask = K.cast(isMask, dtype=K.floatx())\n",
    "    isMask = 1 - isMask #now mask values are zero, and others are 1\n",
    "    \n",
    "    #multiply this by the inputs:\n",
    "    #maybe you might need K.expand_dims(isMask) to add the extra dimension removed by K.all\n",
    "    yTrue = yTrue * isMask  \n",
    "    yPred = yPred * isMask\n",
    "    \n",
    "    def _logcosh(x):\n",
    "        return x + nn.softplus(-2. * x) - math_ops.log(2.)\n",
    "\n",
    "    return K.mean(_logcosh(yPred - yTrue), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_method= \"mean-average_last_next_values\"\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "        \"datasets/datos-demograficos-visitas-tratamientos-missing-imputation-{method}-allnumerics.csv\".format(\n",
    "            method=imputation_method))\n",
    "\n",
    "dataset_original = pd.read_csv(\n",
    "        \"datasets/datos-demograficos-visitas-tratamientos-missing-imputation-{method}.csv\".format(\n",
    "            method=imputation_method))\n",
    "\n",
    "num_features = dataset.shape[1] - 1  # Id is not considered\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset_original[\"Filtrado_glomerular\"].values.reshape(-1,1))\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data, buckets = transform_dataset_different_timesteps(\n",
    "        dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 5\n",
    "nsamples= len(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 70)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_layer (LSTM)            [(None, 256), (None, 334848      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_layer (LSTM)            [(None, None, 256),  264192      decoder_input[0][0]              \n",
      "                                                                 encoder_layer[0][1]              \n",
      "                                                                 encoder_layer[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, None, 1)      257         decoder_layer[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 599,297\n",
      "Trainable params: 599,297\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_features), name=\"encoder_input\")\n",
    "encoder = LSTM(latent_dim, return_state=True, name= \"encoder_layer\")\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard encoder_outputs and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None, 1), name= \"decoder_input\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,  name= \"decoder_layer\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(1, name=\"dense_layer\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# encoder_input_data & decoder_input_data into decoder_target_data\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5591/5591 [==============================] - 176s 31ms/step - loss: 0.1157\n",
      "Epoch 2/5\n",
      "5591/5591 [==============================] - 165s 30ms/step - loss: 0.1009\n",
      "Epoch 3/5\n",
      "5591/5591 [==============================] - 171s 31ms/step - loss: 0.0910\n",
      "Epoch 4/5\n",
      "5591/5591 [==============================] - 163s 29ms/step - loss: 0.0821\n",
      "Epoch 5/5\n",
      "5591/5591 [==============================] - 164s 29ms/step - loss: 0.0734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9233569e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"logcosh\", optimizer='adam')\n",
    "model.fit_generator(generator(encoder_input_data, decoder_input_data, decoder_target_data, num_features), steps_per_epoch = nsamples, epochs = nepochs, use_multiprocessing= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(1)])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 500\n",
    "\n",
    "enc_x = encoder_input_data[i]  # get the list representing the predictive vars\n",
    "dec_x = decoder_input_data[i]\n",
    "dec_y = decoder_target_data[i]\n",
    "\n",
    "timesteps_encoder = int(len(enc_x) / num_features)\n",
    "timesteps_decoder = len(dec_x)\n",
    "\n",
    "enc_x = np.array(enc_x).reshape((1, timesteps_encoder, num_features))\n",
    "dec_x = np.array(dec_x).reshape((1, timesteps_decoder, 1))\n",
    "dec_y = np.array(dec_y).reshape((1, timesteps_decoder, 1))\n",
    "        \n",
    "prediction = model.predict([enc_x, dec_x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26.373783],\n",
       "       [25.309649],\n",
       "       [24.434767]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
