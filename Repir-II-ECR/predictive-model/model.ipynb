{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f0bad229be0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_different_timesteps(dataset):\n",
    "    \"\"\"\n",
    "    dataset: is the original dataset with all numeric columns\n",
    "\n",
    "    This function returns a tuple of lenght 4.\n",
    "    encoder_input_data: list of list\n",
    "    decoder_input_data: list of FG values\n",
    "    decoder_target_data: list of FG values to predict\n",
    "    buckets: dictionary with info of samples with the same timesteps. Key is the number of timesteps, \n",
    "    and value is a list with the indexes of the samples which have this number of timesteps\"\"\"\n",
    "\n",
    "    # Represents the encoder input data for each sample\n",
    "    encoder_input_data = []\n",
    "    # Represents the decoder input data for each sample\n",
    "    decoder_input_data= []\n",
    "    # Represents the decoder target data for each sample\n",
    "    decoder_target_data= []\n",
    "\n",
    "    # Each entry of this list represents the FG value to be predicted for each example\n",
    "    y = []\n",
    "\n",
    "    buckects = {}\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    # For each patient\n",
    "    for index, rows in dataset.groupby(\"Id\"):\n",
    "\n",
    "        if rows.shape == 1:\n",
    "            continue\n",
    "\n",
    "        # Select all columns except the column 'Id'\n",
    "        rows = rows.iloc[:, 1:]\n",
    "\n",
    "        num_visits = rows.shape[0]\n",
    "\n",
    "        for visit in range(1, num_visits):\n",
    "\n",
    "            visit_data = []\n",
    "\n",
    "            rows.iloc[:visit, :].apply(\n",
    "                lambda row: visit_data.extend(list(row.values)), axis=1)\n",
    "            \n",
    "            # El decoder recibira el ultimo FG conocido. Es decir el primer FG pasado al decoder,\n",
    "            # es el ultimo valor de FG que se le pasa al encoder (el Ãºltimo FG de visita). El resto de valores de FG no son pasados al encoder.\n",
    "            \n",
    "            FG_values = rows[\"Filtrado_glomerular\"].values[(visit-1):].tolist()\n",
    "\n",
    "            encoder_input_data.append(visit_data)\n",
    "            # El decoder_input_data tendra los valores de FG que se quieren predecir +1, el adicional es el ultimo \n",
    "            #valor conocido de FG\n",
    "            decoder_input_data.append(FG_values[:-1])\n",
    "            # El decoder_target_data tendra los valores de FG que se quieren predecir\n",
    "            decoder_target_data.append(FG_values[1:])\n",
    "            \n",
    "            key = (visit, len(FG_values)-1)\n",
    "            if key not in buckects:\n",
    "                buckects[key] = [pos]\n",
    "            else:\n",
    "                buckects[key].append(pos)\n",
    "\n",
    "            pos += 1\n",
    "            \n",
    "    #decoder_target_data = sequence.pad_sequences(decoder_target_data, dtype=\"float32\", padding='post', value= -1000).tolist()\n",
    "\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data, buckects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(encoder_X, decoder_X, decoder_y, num_features):\n",
    "\n",
    "    i = 0\n",
    "    nsamples = len(encoder_X)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        enc_x = encoder_X[i]  # get the list representing the predictive vars\n",
    "        dec_x = decoder_X[i]\n",
    "        dec_y = decoder_y[i]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        timesteps_encoder = int(len(enc_x) / num_features)\n",
    "        timesteps_decoder = len(dec_x)\n",
    "\n",
    "        enc_x = np.array(enc_x).reshape((1, timesteps_encoder, num_features))\n",
    "        dec_x = np.array(dec_x).reshape((1, timesteps_decoder, 1))\n",
    "        dec_y = np.array(dec_y).reshape((1, timesteps_decoder, 1))\n",
    "        \n",
    "        yield [enc_x, dec_x], dec_y\n",
    "\n",
    "        # Reset the counter\n",
    "        if i == nsamples:\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_encoder, x_decoder, y_decoder, batch_size, num_features, buckets):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_features = num_features\n",
    "        self.X_encoder = []\n",
    "        self.X_decoder = []\n",
    "        self.y_decoder = []\n",
    "\n",
    "        for key, list_indexes in buckets.items():\n",
    "\n",
    "            permut = np.random.permutation(list_indexes)\n",
    "\n",
    "            split_indices = [int(self.batch_size*(i+1))\n",
    "                             for i in np.arange(np.floor(permut.size / self.batch_size))]\n",
    "\n",
    "            list_arrays = np.array_split(permut, split_indices)\n",
    "\n",
    "            for l in list_arrays:\n",
    "\n",
    "                if len(l) == 0:\n",
    "                    break\n",
    "\n",
    "                x_encoder_array = []\n",
    "                x_decoder_array = []\n",
    "                y_decoder_array = []\n",
    "\n",
    "                for i in l:\n",
    "                    x_encoder_array.append(x_encoder[i])\n",
    "                    x_decoder_array.append(x_decoder[i])\n",
    "                    y_decoder_array.append(y_decoder[i])\n",
    "\n",
    "                x_encoder_array = np.array(x_encoder_array).reshape(\n",
    "                    (l.size, key[0], self.num_features))\n",
    "                x_decoder_array = np.array(x_decoder_array).reshape(\n",
    "                    (l.size, key[1], 1))\n",
    "                y_decoder_array = np.array(y_decoder_array).reshape(\n",
    "                    (l.size, key[1], 1))\n",
    "\n",
    "                self.X_encoder.append(x_encoder_array)\n",
    "                self.X_decoder.append(x_decoder_array)\n",
    "                self.y_decoder.append(y_decoder_array)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_decoder)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.y_decoder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.X_encoder[idx].shape[0] == 0:\n",
    "            import sys\n",
    "            sys.exit(\"Alert: Batch of size 0\")\n",
    "\n",
    "        return [self.X_encoder[idx], self.X_decoder[idx]], self.y_decoder[idx]\n",
    "\n",
    "    def subset(self, indexes):\n",
    "\n",
    "        empty_data = DataSequence(\n",
    "            [], [], [], self.batch_size, self.num_features, {})\n",
    "\n",
    "        for index in indexes:\n",
    "\n",
    "            [X_encoder_temp, X_decoder_temp], y_decoder_temp = self[index]\n",
    "\n",
    "            empty_data.X_encoder.append(X_encoder_temp)\n",
    "            empty_data.X_decoder.append(X_decoder_temp)\n",
    "            empty_data.y_decoder.append(y_decoder_temp)\n",
    "\n",
    "        return empty_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskValue= -1000\n",
    "\n",
    "def custom_loss(yTrue,yPred):\n",
    "    \n",
    "    #find which values in yTrue (target) are the mask value\n",
    "    isMask = K.equal(yTrue, maskValue) #true for all mask values\n",
    "\n",
    "    #transform to float (0 or 1) and invert\n",
    "    isMask = K.cast(isMask, dtype=K.floatx())\n",
    "    isMask = 1 - isMask #now mask values are zero, and others are 1\n",
    "    \n",
    "    #multiply this by the inputs:\n",
    "    #maybe you might need K.expand_dims(isMask) to add the extra dimension removed by K.all\n",
    "    yTrue = yTrue * isMask  \n",
    "    yPred = yPred * isMask\n",
    "    \n",
    "    def _logcosh(x):\n",
    "        return x + nn.softplus(-2. * x) - math_ops.log(2.)\n",
    "\n",
    "    return K.mean(_logcosh(yPred - yTrue), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_method= \"mean-average_last_next_values\"\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "        \"datasets/datos-demograficos-visitas-tratamientos-missing-imputation-{method}-allnumerics.csv\".format(\n",
    "            method=imputation_method))\n",
    "\n",
    "dataset_original = pd.read_csv(\n",
    "        \"datasets/datos-demograficos-visitas-tratamientos-missing-imputation-{method}.csv\".format(\n",
    "            method=imputation_method))\n",
    "\n",
    "num_features = dataset.shape[1] - 1  # Id is not considered\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset_original[\"Filtrado_glomerular\"].values.reshape(-1,1))\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_target_data, buckets = transform_dataset_different_timesteps(\n",
    "        dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 10\n",
    "nsamples= len(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "def create_model(print_summary = False, plot_model = False):\n",
    "    latent_dim = 256\n",
    "\n",
    "    encoder_inputs = Input(shape=(None, num_features), name=\"encoder_input\")\n",
    "    encoder = LSTM(latent_dim, return_state=True, name= \"encoder_layer\")\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    # We discard encoder_outputs and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using encoder_states as initial state.\n",
    "    decoder_inputs = Input(shape=(None, 1), name= \"decoder_input\")\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the \n",
    "    # return states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,  name= \"decoder_layer\")\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states) \n",
    "   \n",
    "    decoder_dense = Dense(1, name=\"dense_layer\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model that will turn\n",
    "    # encoder_input_data & decoder_input_data into decoder_target_data\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    if print_summary:\n",
    "        print(model.summary())\n",
    "        \n",
    "    model.compile(loss=\"logcosh\", optimizer='adam')\n",
    "    \n",
    "    if plot_model:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def fit_model_generator(model):\n",
    "    return model.fit_generator(generator(encoder_input_data, decoder_input_data, decoder_target_data, num_features), steps_per_epoch = nsamples, epochs = nepochs, use_multiprocessing= False)\n",
    "    \n",
    "def fit_model_sequence(model, dataTrain, dataTest, nepochs):\n",
    "    return model.fit_generator(dataTrain, epochs=nepochs, verbose=1, validation_data= dataTest, use_multiprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of prediction with the trained model\n",
    "i = 15\n",
    "\n",
    "enc_x = encoder_input_data[i]  # get the list representing the predictive vars\n",
    "dec_x = decoder_input_data[i]\n",
    "dec_y = decoder_target_data[i]\n",
    "\n",
    "timesteps_encoder = int(len(enc_x) / num_features)\n",
    "timesteps_decoder = len(dec_x)\n",
    "\n",
    "enc_x = np.array(enc_x).reshape((1, timesteps_encoder, num_features))\n",
    "dec_x = np.array(dec_x).reshape((1, timesteps_decoder, 1))\n",
    "dec_y = np.array(dec_y).reshape((1, timesteps_decoder, 1))\n",
    "        \n",
    "prediction = model.predict([enc_x, dec_x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[63.0442  ],\n",
       "       [63.429577]], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
