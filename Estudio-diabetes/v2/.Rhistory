row.names = FALSE, col.names = FALSE)
png(paste(fileName,"-VarImp.png", sep = ""), width = 8, height = 6, units="in", res=200)
viplot(newData$Importance, xlab = as.vector(newData$Name),
col=heat.colors(length(newData$Importance)), xlab.mar = 7)
dev.off()
library(FactoMineR)
data(decathlon)
res = PCA(decathlon, quanti.sup = 11:12, quali.sup=13, graph=FALSE)
Investigate(res)
install.packages("FactoMineR")
library(FactoMineR)
data(decathlon)
View(decathlon)
res = PCA(decathlon, quanti.sup = 11:12, quali.sup=13, graph=FALSE)
res
Investigate(res)
Investigate(res)
install.packages("FactoInvestigate")
install.packages("FactoInvestigate")
install.packages("FactoInvestigate")
Investigate(res)
Investigate(res)
library(FactoInvestigate)
Investigate(res)
?Investigate
View(database)
View(dataset)
res = PCA(dataset, quali.sup=5, graph=FALSE)
Investigate(res,document = "pdf_document")
Investigate(dataset,document = "pdf_document")
Investigate(res,document = "pdf_document")
View(decathlon)
View(database)
View(databaseTransformed)
View(dataset)
rownames(dataset)
res <- PCA(dataset, quali.sup=5, graph=FALSE)
res <- PCA(dataset, quali.sup=5, graph=TRIE)
res <- PCA(dataset, quali.sup=5, graph=TRUE)
plot(res)
Investigate(res)
Investigate(res,document = "word_document")
?PCA
res <- PCA(dataset, quali.sup=5, graph=FALSE, scale.unit = FALSE)
Investigate(res)
res
fviz_screeplot(res)
install.packages("factoextra")
library("factoextra")
fviz_screeplot(res)
var <- get_pca_var(res)
head(var$coord)
head(var$contrib)
fviz_pca_var(res, col.var = "black")
# Graph of individuals
# 1. Use repel = TRUE to avoid overplotting
# 2. Control automatically the color of individuals using the cos2
# cos2 = the quality of the individuals on the factor map
# Use points only
# 3. Use gradient color
fviz_pca_ind(res, col.ind = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE # Avoid text overlapping (slow if many points)
)
fviz_pca_var(res)
fviz_pca_var(res, col.var="contrib")
fviz_pca_ind(res)
fviz_pca_ind(res,label="none", habillage=dataset$Cancer)
fviz_pca_ind(res,label="none", habillage=dataset$Cancer, addEllipses=TRUE, ellipse.level=0.95)
fviz_pca_biplot(res,
habillage = dataset$Cancer, addEllipses = TRUE,
col.var = "red", alpha.var ="cos2",
label = "var") +
scale_color_brewer(palette="Dark2")+
theme_minimal()
dimdesc(res, axes = 1:3, proba = 0.05)
install.packages("Factoshiny")
library(Factoshiny)
PCAshiny(res)
HCPCshiny(res)
data <- read.csv("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/FC-preproc.csv")
View(data)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
library(caret)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
outputFolder <- "results/FC/"
#Not available in Windows
# library(doMC)
# registerDoMC(cores = 5)
## All subsequent models are then run in parallel
#Required packages
# in debian-8, run the command sudo apt-get install r-cran-rjava
# sudo R CMD javareconf JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/jar
wants <- c("ada","adabag", "bartMachine", "earth", "gbm", "glmnet",
"C50", "caTools", "fastAdaboost", "import",
"mboost", "mda", "MASS", "Matrix", "Metrics", "msaenet", "nodeHarvest",
"party", "pamr", "plyr",
"RWeka", "ROCR", "rFerns", "randomForest", "RRF",
"sdwd", "sparseLDA", "xgboost", "wsrf")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
# Able to handle multi-class classification
# **********************************************
# Method name - Full name - packages
# AdaBoost.M1 - AdaBoost.M1 - adabag, plyr
# AdaBag - Bagged AdaBoost - adabag, plyr
# bagFDA - Bagged Flexible Discriminant Analysis - earth, mda
# bagEarth - Bagged MARS - earth
# LogitBoost - Boosted Logistic Regression - caTools
# J48 - C4.5-like Trees - RWeka
# C5.0 - C5.0 - C50, plyr
# cforest - Conditional Inference Random Forest - party
# ctree - Conditional Inference Tree - party
# xgbLinear - eXtreme Gradient Boosting - xgboost
# xgbTree- eXtreme Gradient Boosting - xgboost
# fda - Flexible Discriminant Analysis - earth, mda
# glmnet - glmnet- glmnet, Matrix
# LMT - Logistic Model Trees - RWeka
# msaenet - Multi-Step Adaptive MCP-Net - msaenet
# earth - Multivariate Adaptive Regression Spline - earth
# pam - Nearest Shrunken Centroids - pamr
# RRF - Regularized Random Forest - randomForest, RRF
# JRip - Rule-Based Classifier - RWeka
# PART - Rule-Based Classifier - RWeka
# OneR - Single Rule Classification - RWeka
# sdwd - Sparse Distance Weighted Discrimination - sdwd
# sparseLDA - Sparse Linear Discriminant Analysis - sparseLDA
# gbm - Stochastic Gradient Boosting - gbm, plyr
# wsrf - Weighted Subspace Random Forest - wsrf
# Two class only
# **********************************************
# Method name - Full name - packages
# adaboost - AdaBoost Classification Trees - fastAdaboost
# bartMachine - Bayesian Additive Regression Trees - bartMachine
# ada - Boosted Classification Trees - ada, plyr
# gamboost - Boosted Generalized Additive Model - mboost, plyr, import
# glmStepAIC - Generalized Linear Model with Stepwise Feature Selection - MASS
# nodeHarvest - Tree-Based Ensembles- nodeHarvest
algorithmsAbleMulti <- c("C5.0", "cforest",
"ctree", "xgbLinear", "xgbTree", "fda", "glmnet",
"msaenet", "earth", "pam",
"RRF", "sdwd", "sparseLDA", "gbm", "wsrf",
"AdaBoost.M1", "AdaBag", "bagFDA", "bagEarth", "LogitBoost")
algorithmsTwoClassesOnly <- c("adaboost", "ada", "gamboost", "glmStepAIC",
"nodeHarvest")
mydata <- read.csv("FC-preproc.csv")
numberClasses <- length(levels(mydata[,ncol(mydata)]))
algorithms <- algorithmsAbleMulti
metricOpt <- "AUC"
summaryFunction <- multiClassSummary
if(numberClasses == 2)
{
algorithms <- c(algorithms, algorithmsTwoClassesOnly)
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
}
set.seed(123)
#we use an automatic grid by means of using the parameter tunelength
#see http://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, search="grid", allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
form <- as.formula(paste(colnames(mydata)[ncol(mydata)]," ~ ."))
#execute the algorithms
for(algorithm in algorithms){
modelFit <- train(form, data = mydata,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 15,
trControl = fitControl)
#Saving all the results
write.table(modelFit$results, file= paste(outputFolder,"results-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
#Saving the variable importance
varImportance <- varImp(modelFit, scale= TRUE)
write.table(varImportance$importance, file= paste(outputFolder,"varImportance-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = TRUE, col.names = TRUE, na = "")
#Saving the model for graphing plots a posteriory.
# save the model to disk
saveRDS(modelFit, paste(outputFolder,"modelfit-",algorithm, ".rds", sep = ""))
# To save which predictors were used in the final model.
write.table(data.frame(Predictor = predictors(modelFit)), file= paste(outputFolder,"predictors-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = FALSE, na = "")
}
warnings()
library(caret)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
outputFolder <- "results/FC/"
#Not available in Windows
# library(doMC)
# registerDoMC(cores = 5)
## All subsequent models are then run in parallel
#Required packages
# in debian-8, run the command sudo apt-get install r-cran-rjava
# sudo R CMD javareconf JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/jar
wants <- c("ada","adabag", "bartMachine", "earth", "gbm", "glmnet",
"C50", "caTools", "fastAdaboost", "import",
"mboost", "mda", "MASS", "Matrix", "Metrics", "msaenet", "nodeHarvest",
"party", "pamr", "plyr",
"RWeka", "ROCR", "rFerns", "randomForest", "RRF",
"sdwd", "sparseLDA", "xgboost", "wsrf")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
# Able to handle multi-class classification
# **********************************************
# Method name - Full name - packages
# AdaBoost.M1 - AdaBoost.M1 - adabag, plyr
# AdaBag - Bagged AdaBoost - adabag, plyr
# bagFDA - Bagged Flexible Discriminant Analysis - earth, mda
# bagEarth - Bagged MARS - earth
# LogitBoost - Boosted Logistic Regression - caTools
# J48 - C4.5-like Trees - RWeka
# C5.0 - C5.0 - C50, plyr
# cforest - Conditional Inference Random Forest - party
# ctree - Conditional Inference Tree - party
# xgbLinear - eXtreme Gradient Boosting - xgboost
# xgbTree- eXtreme Gradient Boosting - xgboost
# fda - Flexible Discriminant Analysis - earth, mda
# glmnet - glmnet- glmnet, Matrix
# LMT - Logistic Model Trees - RWeka
# msaenet - Multi-Step Adaptive MCP-Net - msaenet
# earth - Multivariate Adaptive Regression Spline - earth
# pam - Nearest Shrunken Centroids - pamr
# RRF - Regularized Random Forest - randomForest, RRF
# JRip - Rule-Based Classifier - RWeka
# PART - Rule-Based Classifier - RWeka
# OneR - Single Rule Classification - RWeka
# sdwd - Sparse Distance Weighted Discrimination - sdwd
# sparseLDA - Sparse Linear Discriminant Analysis - sparseLDA
# gbm - Stochastic Gradient Boosting - gbm, plyr
# wsrf - Weighted Subspace Random Forest - wsrf
# Two class only
# **********************************************
# Method name - Full name - packages
# adaboost - AdaBoost Classification Trees - fastAdaboost
# bartMachine - Bayesian Additive Regression Trees - bartMachine
# ada - Boosted Classification Trees - ada, plyr
# gamboost - Boosted Generalized Additive Model - mboost, plyr, import
# glmStepAIC - Generalized Linear Model with Stepwise Feature Selection - MASS
# nodeHarvest - Tree-Based Ensembles- nodeHarvest
algorithmsAbleMulti <- c("cforest",
"ctree", "xgbLinear", "xgbTree", "fda", "glmnet",
"msaenet", "earth", "pam",
"RRF", "sdwd", "sparseLDA", "gbm", "wsrf",
"AdaBoost.M1", "AdaBag", "bagFDA", "bagEarth", "LogitBoost")
algorithmsTwoClassesOnly <- c("adaboost", "ada", "gamboost", "glmStepAIC",
"nodeHarvest")
mydata <- read.csv("FC-preproc.csv")
numberClasses <- length(levels(mydata[,ncol(mydata)]))
algorithms <- algorithmsAbleMulti
metricOpt <- "AUC"
summaryFunction <- multiClassSummary
if(numberClasses == 2)
{
algorithms <- c(algorithms, algorithmsTwoClassesOnly)
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
}
set.seed(123)
#we use an automatic grid by means of using the parameter tunelength
#see http://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, search="grid", allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
form <- as.formula(paste(colnames(mydata)[ncol(mydata)]," ~ ."))
#execute the algorithms
for(algorithm in algorithms){
modelFit <- train(form, data = mydata,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 15,
trControl = fitControl)
#Saving all the results
write.table(modelFit$results, file= paste(outputFolder,"results-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
#Saving the variable importance
varImportance <- varImp(modelFit, scale= TRUE)
write.table(varImportance$importance, file= paste(outputFolder,"varImportance-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = TRUE, col.names = TRUE, na = "")
#Saving the model for graphing plots a posteriory.
# save the model to disk
saveRDS(modelFit, paste(outputFolder,"modelfit-",algorithm, ".rds", sep = ""))
# To save which predictors were used in the final model.
write.table(data.frame(Predictor = predictors(modelFit)), file= paste(outputFolder,"predictors-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = FALSE, na = "")
}
library(caret)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
outputFolder <- "results/FC/"
#Not available in Windows
# library(doMC)
# registerDoMC(cores = 5)
## All subsequent models are then run in parallel
#Required packages
# in debian-8, run the command sudo apt-get install r-cran-rjava
# sudo R CMD javareconf JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/jar
wants <- c("ada","adabag", "bartMachine", "earth", "gbm", "glmnet",
"C50", "caTools", "fastAdaboost", "import",
"mboost", "mda", "MASS", "Matrix", "Metrics", "msaenet", "nodeHarvest",
"party", "pamr", "plyr",
"RWeka", "ROCR", "rFerns", "randomForest", "RRF",
"sdwd", "sparseLDA", "xgboost", "wsrf")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
# Able to handle multi-class classification
# **********************************************
# Method name - Full name - packages
# AdaBoost.M1 - AdaBoost.M1 - adabag, plyr
# AdaBag - Bagged AdaBoost - adabag, plyr
# bagFDA - Bagged Flexible Discriminant Analysis - earth, mda
# bagEarth - Bagged MARS - earth
# LogitBoost - Boosted Logistic Regression - caTools
# J48 - C4.5-like Trees - RWeka
# C5.0 - C5.0 - C50, plyr
# cforest - Conditional Inference Random Forest - party
# ctree - Conditional Inference Tree - party
# xgbLinear - eXtreme Gradient Boosting - xgboost
# xgbTree- eXtreme Gradient Boosting - xgboost
# fda - Flexible Discriminant Analysis - earth, mda
# glmnet - glmnet- glmnet, Matrix
# LMT - Logistic Model Trees - RWeka
# msaenet - Multi-Step Adaptive MCP-Net - msaenet
# earth - Multivariate Adaptive Regression Spline - earth
# pam - Nearest Shrunken Centroids - pamr
# RRF - Regularized Random Forest - randomForest, RRF
# JRip - Rule-Based Classifier - RWeka
# PART - Rule-Based Classifier - RWeka
# OneR - Single Rule Classification - RWeka
# sdwd - Sparse Distance Weighted Discrimination - sdwd
# sparseLDA - Sparse Linear Discriminant Analysis - sparseLDA
# gbm - Stochastic Gradient Boosting - gbm, plyr
# wsrf - Weighted Subspace Random Forest - wsrf
# Two class only
# **********************************************
# Method name - Full name - packages
# adaboost - AdaBoost Classification Trees - fastAdaboost
# bartMachine - Bayesian Additive Regression Trees - bartMachine
# ada - Boosted Classification Trees - ada, plyr
# gamboost - Boosted Generalized Additive Model - mboost, plyr, import
# glmStepAIC - Generalized Linear Model with Stepwise Feature Selection - MASS
# nodeHarvest - Tree-Based Ensembles- nodeHarvest
algorithmsAbleMulti <- c("cforest",
"ctree", "xgbLinear", "xgbTree", "fda", "glmnet",
"msaenet", "earth", "pam",
"RRF", "sdwd", "sparseLDA", "gbm", "wsrf",
"AdaBoost.M1", "AdaBag", "bagFDA", "bagEarth", "LogitBoost")
algorithmsTwoClassesOnly <- c("adaboost", "ada", "gamboost", "glmStepAIC",
"nodeHarvest")
mydata <- read.csv("FC-preproc.csv")
numberClasses <- length(levels(mydata[,ncol(mydata)]))
algorithms <- algorithmsAbleMulti
metricOpt <- "AUC"
summaryFunction <- multiClassSummary
if(numberClasses == 2)
{
algorithms <- c(algorithms, algorithmsTwoClassesOnly)
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
}
set.seed(123)
#we use an automatic grid by means of using the parameter tunelength
#see http://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, search="random", allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
form <- as.formula(paste(colnames(mydata)[ncol(mydata)]," ~ ."))
#execute the algorithms
for(algorithm in algorithms){
modelFit <- train(form, data = mydata,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 15,
trControl = fitControl)
#Saving all the results
write.table(modelFit$results, file= paste(outputFolder,"results-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
#Saving the variable importance
varImportance <- varImp(modelFit, scale= TRUE)
write.table(varImportance$importance, file= paste(outputFolder,"varImportance-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = TRUE, col.names = TRUE, na = "")
#Saving the model for graphing plots a posteriory.
# save the model to disk
saveRDS(modelFit, paste(outputFolder,"modelfit-",algorithm, ".rds", sep = ""))
# To save which predictors were used in the final model.
write.table(data.frame(Predictor = predictors(modelFit)), file= paste(outputFolder,"predictors-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = FALSE, na = "")
}
library(ggplot2)
library(plotROC)
library(pROC)
library(ROCR)
library(caret)
library(bclust)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
output <- "results/0horas/"
library(ggplot2)
library(plotROC)
library(pROC)
library(ROCR)
library(caret)
library(bclust)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
output <- "results/0horas/"
library(caret)
setwd("D:/OneDrive - Universidad de C贸rdoba/workspace/IMIBIC/datasets/diabetes/v2/")
outputFolder <- "results/FC/"
#Not available in Windows
# library(doMC)
# registerDoMC(cores = 5)
## All subsequent models are then run in parallel
#Required packages
# in debian-8, run the command sudo apt-get install r-cran-rjava
# sudo R CMD javareconf JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/jar
wants <- c("ada","adabag", "bartMachine", "earth", "gbm", "glmnet",
"C50", "caTools", "fastAdaboost", "import",
"mboost", "mda", "MASS", "Matrix", "Metrics", "msaenet", "nodeHarvest",
"party", "pamr", "plyr",
"RWeka", "ROCR", "rFerns", "randomForest", "RRF",
"sdwd", "sparseLDA", "xgboost", "wsrf")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
# Able to handle multi-class classification
# **********************************************
# Method name - Full name - packages
# AdaBoost.M1 - AdaBoost.M1 - adabag, plyr
# AdaBag - Bagged AdaBoost - adabag, plyr
# bagFDA - Bagged Flexible Discriminant Analysis - earth, mda
# bagEarth - Bagged MARS - earth
# LogitBoost - Boosted Logistic Regression - caTools
# J48 - C4.5-like Trees - RWeka
# C5.0 - C5.0 - C50, plyr
# cforest - Conditional Inference Random Forest - party
# ctree - Conditional Inference Tree - party
# xgbLinear - eXtreme Gradient Boosting - xgboost
# xgbTree- eXtreme Gradient Boosting - xgboost
# fda - Flexible Discriminant Analysis - earth, mda
# glmnet - glmnet- glmnet, Matrix
# LMT - Logistic Model Trees - RWeka
# msaenet - Multi-Step Adaptive MCP-Net - msaenet
# earth - Multivariate Adaptive Regression Spline - earth
# pam - Nearest Shrunken Centroids - pamr
# RRF - Regularized Random Forest - randomForest, RRF
# JRip - Rule-Based Classifier - RWeka
# PART - Rule-Based Classifier - RWeka
# OneR - Single Rule Classification - RWeka
# sdwd - Sparse Distance Weighted Discrimination - sdwd
# sparseLDA - Sparse Linear Discriminant Analysis - sparseLDA
# gbm - Stochastic Gradient Boosting - gbm, plyr
# wsrf - Weighted Subspace Random Forest - wsrf
# Two class only
# **********************************************
# Method name - Full name - packages
# adaboost - AdaBoost Classification Trees - fastAdaboost
# bartMachine - Bayesian Additive Regression Trees - bartMachine
# ada - Boosted Classification Trees - ada, plyr
# gamboost - Boosted Generalized Additive Model - mboost, plyr, import
# glmStepAIC - Generalized Linear Model with Stepwise Feature Selection - MASS
# nodeHarvest - Tree-Based Ensembles- nodeHarvest
algorithmsAbleMulti <- c(#"cforest", "ctree", "xgbLinear", "xgbTree",
"fda", "glmnet",
"msaenet", "earth", "pam",
"RRF", "sdwd", "sparseLDA", "gbm", "wsrf",
"AdaBoost.M1", "AdaBag", "bagFDA", "bagEarth", "LogitBoost")
algorithmsTwoClassesOnly <- c("adaboost", "ada", "gamboost", "glmStepAIC",
"nodeHarvest")
mydata <- read.csv("FC-preproc.csv")
numberClasses <- length(levels(mydata[,ncol(mydata)]))
algorithms <- algorithmsAbleMulti
metricOpt <- "AUC"
summaryFunction <- multiClassSummary
if(numberClasses == 2)
{
algorithms <- c(algorithms, algorithmsTwoClassesOnly)
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
}
set.seed(123)
#we use an automatic grid by means of using the parameter tunelength
#see http://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, search="random", allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
form <- as.formula(paste(colnames(mydata)[ncol(mydata)]," ~ ."))
#execute the algorithms
for(algorithm in algorithms){
modelFit <- train(form, data = mydata,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 15,
trControl = fitControl)
#Saving all the results
write.table(modelFit$results, file= paste(outputFolder,"results-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = TRUE, na = "")
#Saving the variable importance
varImportance <- varImp(modelFit, scale= TRUE)
write.table(varImportance$importance, file= paste(outputFolder,"varImportance-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = TRUE, col.names = TRUE, na = "")
#Saving the model for graphing plots a posteriory.
# save the model to disk
saveRDS(modelFit, paste(outputFolder,"modelfit-",algorithm, ".rds", sep = ""))
# To save which predictors were used in the final model.
write.table(data.frame(Predictor = predictors(modelFit)), file= paste(outputFolder,"predictors-",algorithm, ".csv", sep = ""),
quote = FALSE, sep="," , row.names = FALSE, col.names = FALSE, na = "")
}
