# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "Indices-0h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "Indices-4h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "Indices-0h-4h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "0h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "4h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
library(caret)
library(pROC)
fileName <- "0h-4h"
output <- "results/C4.5/"
dataset <- read.csv(paste(fileName,".csv", sep = ""), sep = ";", row.names = 1)
set.seed(123)
fitControl <- trainControl(method="repeatedcv", number = 10, repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = twoClassSummary, verboseIter = FALSE)
form <- as.formula("DIABETES ~ .")
#pass this dataset to the classification algorithm
modelFit <- train(form, data = dataset,
method="J48",
metric = "ROC",
maximize = TRUE,
trControl = fitControl,
tuneLength = 10)
# See discusion at this url https://stats.stackexchange.com/questions/195264/repeated-crossvalidation-finalmodel-and-roc-curves
# Here different forms to compute the ROC curve are exposed.
#ROC 1. roc.1 is irrelevant as it evaluates a model on the same data used to train it (the finalModel is
#just the fit on Data ignoring the CV argument, built to apply on a different dataset for future prediction)
plsProbs <- predict(modelFit, newdata = dataset, type = "prob") # Predict over the same training data
roc.1 <- roc(dataset$DIABETES, plsProbs$S) # Draw ROC curve.
plot(roc.1, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-overTraining.png', sep = ""))
dev.off()
# Extracting the best tune
predictions <- modelFit$pred[modelFit$pred$C == modelFit$bestTune$C & modelFit$pred$M == modelFit$bestTune$M,]
# ROC 2. roc.2 is 'almost' accurate as it will consider each prediction independently (averaging the
# prediction, not the probabilities)
roc.2 <- roc(predictions$obs, predictions$S) # Draw ROC curve.
plot(roc.2, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-almost-CV.png', sep = ""))
dev.off()
# ROC 3. roc.3 is the correct way to do it as it averages the prediction probabilities for each sample
# among the repeated CV (contrary to roc.2 where the prediction results are averaged)
roc.3 <- roc(as.numeric(modelFit$trainingData$.outcome=='S'), aggregate(S ~ rowIndex, predictions, mean)[,'S'])
plot(roc.3, print.auc = TRUE)
dev.copy(png, paste(output, fileName,'-prediction-correct-CV.png', sep = ""))
dev.off()
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
View(rd)
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
View(rd)
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
View(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
View(rd)
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript("Bergmann-Hommel.eps", width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- TRUE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript("Bergmann-Hommel.eps", width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
mydata <- read.csv(paste(fileName,".csv",sep = ""), sep = ";", row.names = 1)
output <- "results/"
fileName <- "0h"
mydata <- read.csv(paste(fileName,".csv",sep = ""), sep = ";", row.names = 1)
View(mydata)
