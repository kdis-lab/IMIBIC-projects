bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
View(rd)
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
View(rd)
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
View(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
View(rd)
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript(paste(pathFolder, metric, ".eps", sep = ""), width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- FALSE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript("Bergmann-Hommel.eps", width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
#load the library
library(scmamp)
# set whether the measure is maximal or minimal
maximize <- TRUE;
# set the significance level
alpha <- 0.05;
#load the csv file
rd <- read.csv("cv-results.csv", header = TRUE, sep = ";")
nAlgorithms <- ncol(rd)-1
nDatasets <- nrow(rd)
# upper case the first letter of dataset names
rd$Dataset <- paste(toupper(substring(rd$Dataset, 1, 1)), substring(rd$Dataset, 2), sep = "")
#sort the dataframe by dataset name
rd <- rd[order(rd$Dataset),]
rdm <- rd[, 2: (nAlgorithms+1)]
#create latex table
f <- if(maximize) max else min
boldT <- matrix(data=FALSE, nrow = nDatasets, ncol = (nAlgorithms+1))
for(i in 1:nDatasets){
maxmin <- f(rdm[i,])
boldT[i, 2:(nAlgorithms+1)] <- (rdm[i,] == maxmin)
}
boldT[is.na(boldT)] <- FALSE
writeTabular(table = rd, format = 'f', bold = boldT, hrule = 0, vrule = 0, align = 'c', print.row.names=FALSE)
#Friedman test. Multiple comparison
friedman <- friedmanTest(data=rdm,alpha=alpha)
friedman
#Friedman rankings
average.ranking <- colMeans(rankMatrix(rdm, decreasing = maximize))
average.ranking
#If there are sig. differences
if(friedman$p.value < alpha) {
#############Post-hoc tests#################################
#post-Hoc test
res <- postHocTest(data = rdm, test = 'friedman', correct = 'bergmann', use.rank=TRUE)
#table N vs N
bold <- res$corrected.pval < alpha
bold[is.na(bold)] <- FALSE
writeTabular(table = res$corrected.pval, format = 'f', bold = bold, hrule = 0, vrule = 0)
setEPS()
postscript("Bergmann-Hommel.eps", width = 6, height = 4)
# plot
plotRanking(pvalues = res$corrected.pval, summary=average.ranking, decreasing = FALSE, alpha=alpha)
dev.off()
}
library(caret)
library(rJava)
#library(doMC)
#registerDoMC(cores = 8)
threshold <- 0.8
output <- "results/"
fileName <- "0h"
#load the dataset and ranking of features
dataset <-  read.csv(paste(fileName,".csv",sep = ""), sep = ";", row.names = 1)
rankingFeature <- read.csv(paste(output,fileName,"-sortedfeatures.csv", sep = ""), sep = "\t")
nFeatures <- nrow(rankingFeature)
classIndex <- ncol(dataset)
algorithms <- c("J48", "PART", "rf", "glm")
numberClasses <- length(levels(dataset[,ncol(dataset)]))
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
set.seed(123)
form <- as.formula(paste(colnames(dataset)[ncol(dataset)]," ~ ."))
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
#execute the algorithms
for(algorithm in algorithms){
cat(paste0(algorithm, "\n"))
#The dataframe where we stored the results for this algorithm
df <- data.frame(ID = c(0), NumberAtts=c(0), Atts=c(" "), metricOpt=c(0))
df <- df[-1,]
#Execute the search by ranking of features
#######################
#All features will be considered as pivot one time
for(feature in 1:nFeatures){
cat(paste0("\t", feature, "\n"))
pivotFeature <- rankingFeature[feature,"Index"]
listFeatures <- c(pivotFeature)
aucGlobal <- -1
for(otherFeature in feature:nFeatures)
{
cat(paste0("\t\t", otherFeature, "\n"))
# The last case
if(otherFeature != feature)
{
listFeatures <- c(listFeatures, rankingFeature[otherFeature,"Index"])
}
#extract the subset
subdataset <- dataset[, c(listFeatures, classIndex)]
#pass this subdataset to the classification algorithm
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl)
if(algorithm == "glm"){
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl, family="binomial")
}
bestTuneIndex <- as.numeric(rownames(modelFit$bestTune)[1])
auc <- modelFit$results[bestTuneIndex, metricOpt]
#register the model
if(auc>= threshold){
vars <- predictors(modelFit)
df <- rbind(df, data.frame(ID = nrow(df) + 1, NumberAtts= length(vars), Atts= paste(vars, collapse = ' '), metricOpt=auc))
#Saving the model for graphing plots a posteriory if it is necessary
#Cache rJava object classifier in order to save it with the object
.jcache(modelFit$finalModel$classifier)
saveRDS(modelFit, paste(output, algorithm, "/modelfit-",algorithm,"-", nrow(df),".rds", sep = ""))
if(algorithm == "J48"){
pdf(paste(output,algorithm, "/tree-", algorithm,"-", nrow(df),".pdf", sep = ""), width = 12, height = 8)
plot(modelFit$finalModel)
dev.off()
}
else if(algorithm == "PART"){
write(modelFit$finalModel, file = paste(output,algorithm,"/rules-",algorithm,"-", nrow(df),".txt", sep = ""))
}
else if(algorithm == "rf"){
write(paste("rf-", nrow(df),";", modelFit$finalModel$ntree, sep = ""), file = paste(output,algorithm,"/rf.txt", sep = ""), append = TRUE)
}
}
if(aucGlobal < auc){
aucGlobal <- auc
}
else{
#remove the last feature added. due to did not apport any advantage
listFeatures <- listFeatures[1:(length(listFeatures)-1)]
}
}
}
#######################End of search by ranking
#order the rows of the dataframe
df <- df[ order(-df[,ncol(df)]), ]
fileNameT <- paste(output,algorithm,"/", algorithm,".csv")
#save the dataframe
write.table(df, file= fileNameT, quote = FALSE, sep="," , row.names = FALSE,
col.names = TRUE)
}
library(caret)
library(rJava)
#library(doMC)
#registerDoMC(cores = 8)
threshold <- 0.8
output <- "results/"
fileName <- "0h"
#load the dataset and ranking of features
dataset <-  read.csv(paste(fileName,".csv",sep = ""), sep = ";", row.names = 1)
rankingFeature <- read.csv(paste(output,fileName,"-sortedfeatures.csv", sep = ""), sep = "\t")
nFeatures <- nrow(rankingFeature)
classIndex <- ncol(dataset)
algorithms <- c("glm", "J48", "PART", "rf")
numberClasses <- length(levels(dataset[,ncol(dataset)]))
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
set.seed(123)
form <- as.formula(paste(colnames(dataset)[ncol(dataset)]," ~ ."))
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
#execute the algorithms
for(algorithm in algorithms){
cat(paste0(algorithm, "\n"))
#The dataframe where we stored the results for this algorithm
df <- data.frame(ID = c(0), NumberAtts=c(0), Atts=c(" "), metricOpt=c(0))
df <- df[-1,]
#Execute the search by ranking of features
#######################
#All features will be considered as pivot one time
for(feature in 1:nFeatures){
cat(paste0("\t", feature, "\n"))
pivotFeature <- rankingFeature[feature,"Index"]
listFeatures <- c(pivotFeature)
aucGlobal <- -1
for(otherFeature in feature:nFeatures)
{
cat(paste0("\t\t", otherFeature, "\n"))
# The last case
if(otherFeature != feature)
{
listFeatures <- c(listFeatures, rankingFeature[otherFeature,"Index"])
}
#extract the subset
subdataset <- dataset[, c(listFeatures, classIndex)]
#pass this subdataset to the classification algorithm
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl)
if(algorithm == "glm"){
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl, family="binomial")
}
bestTuneIndex <- as.numeric(rownames(modelFit$bestTune)[1])
auc <- modelFit$results[bestTuneIndex, metricOpt]
#register the model
if(auc>= threshold){
vars <- predictors(modelFit)
df <- rbind(df, data.frame(ID = nrow(df) + 1, NumberAtts= length(vars), Atts= paste(vars, collapse = ' '), metricOpt=auc))
#Saving the model for graphing plots a posteriory if it is necessary
#Cache rJava object classifier in order to save it with the object
.jcache(modelFit$finalModel$classifier)
saveRDS(modelFit, paste(output, algorithm, "/modelfit-",algorithm,"-", nrow(df),".rds", sep = ""))
if(algorithm == "J48"){
pdf(paste(output,algorithm, "/tree-", algorithm,"-", nrow(df),".pdf", sep = ""), width = 12, height = 8)
plot(modelFit$finalModel)
dev.off()
}
else if(algorithm == "PART"){
write(modelFit$finalModel, file = paste(output,algorithm,"/rules-",algorithm,"-", nrow(df),".txt", sep = ""))
}
else if(algorithm == "rf"){
write(paste("rf-", nrow(df),";", modelFit$finalModel$ntree, sep = ""), file = paste(output,algorithm,"/rf.txt", sep = ""), append = TRUE)
}
}
if(aucGlobal < auc){
aucGlobal <- auc
}
else{
#remove the last feature added. due to did not apport any advantage
listFeatures <- listFeatures[1:(length(listFeatures)-1)]
}
}
}
#######################End of search by ranking
#order the rows of the dataframe
df <- df[ order(-df[,ncol(df)]), ]
fileNameT <- paste(output,algorithm,"/", algorithm,".csv")
#save the dataframe
write.table(df, file= fileNameT, quote = FALSE, sep="," , row.names = FALSE,
col.names = TRUE)
}
library(caret)
library(rJava)
#library(doMC)
#registerDoMC(cores = 8)
threshold <- 0.8
output <- "results/"
fileName <- "0h"
#load the dataset and ranking of features
dataset <-  read.csv(paste(fileName,".csv",sep = ""), sep = ";", row.names = 1)
rankingFeature <- read.csv(paste(output,fileName,"-sortedfeatures.csv", sep = ""), sep = "\t")
nFeatures <- nrow(rankingFeature)
classIndex <- ncol(dataset)
algorithms <- c("rf", "glm", "J48", "PART")
numberClasses <- length(levels(dataset[,ncol(dataset)]))
metricOpt <- "ROC"
summaryFunction <- twoClassSummary
set.seed(123)
form <- as.formula(paste(colnames(dataset)[ncol(dataset)]," ~ ."))
fitControl <- trainControl(method="repeatedcv", number = 10,
repeats = 3, classProbs = TRUE,
savePredictions = TRUE, allowParallel= TRUE,
summaryFunction = summaryFunction, verboseIter = FALSE)
#execute the algorithms
for(algorithm in algorithms){
cat(paste0(algorithm, "\n"))
#The dataframe where we stored the results for this algorithm
df <- data.frame(ID = c(0), NumberAtts=c(0), Atts=c(" "), metricOpt=c(0))
df <- df[-1,]
#Execute the search by ranking of features
#######################
#All features will be considered as pivot one time
for(feature in 1:nFeatures){
cat(paste0("\t", feature, "\n"))
pivotFeature <- rankingFeature[feature,"Index"]
listFeatures <- c(pivotFeature)
aucGlobal <- -1
for(otherFeature in feature:nFeatures)
{
cat(paste0("\t\t", otherFeature, "\n"))
# The last case
if(otherFeature != feature)
{
listFeatures <- c(listFeatures, rankingFeature[otherFeature,"Index"])
}
#extract the subset
subdataset <- dataset[, c(listFeatures, classIndex)]
#pass this subdataset to the classification algorithm
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl)
if(algorithm == "glm"){
modelFit <- train(form, data = subdataset,
method=algorithm,
metric = metricOpt,
maximize = TRUE,
tuneLength = 30,
trControl = fitControl, family="binomial")
}
bestTuneIndex <- as.numeric(rownames(modelFit$bestTune)[1])
auc <- modelFit$results[bestTuneIndex, metricOpt]
#register the model
if(auc>= threshold){
vars <- predictors(modelFit)
df <- rbind(df, data.frame(ID = nrow(df) + 1, NumberAtts= length(vars), Atts= paste(vars, collapse = ' '), metricOpt=auc))
#Saving the model for graphing plots a posteriory if it is necessary
#Cache rJava object classifier in order to save it with the object
.jcache(modelFit$finalModel$classifier)
saveRDS(modelFit, paste(output, algorithm, "/modelfit-",algorithm,"-", nrow(df),".rds", sep = ""))
if(algorithm == "J48"){
pdf(paste(output,algorithm, "/tree-", algorithm,"-", nrow(df),".pdf", sep = ""), width = 12, height = 8)
plot(modelFit$finalModel)
dev.off()
}
else if(algorithm == "PART"){
write(modelFit$finalModel, file = paste(output,algorithm,"/rules-",algorithm,"-", nrow(df),".txt", sep = ""))
}
else if(algorithm == "rf"){
write(paste("rf-", nrow(df),";", modelFit$finalModel$ntree, sep = ""), file = paste(output,algorithm,"/rf.txt", sep = ""), append = TRUE)
}
}
if(aucGlobal < auc){
aucGlobal <- auc
}
else{
#remove the last feature added. due to did not apport any advantage
listFeatures <- listFeatures[1:(length(listFeatures)-1)]
}
}
}
#######################End of search by ranking
#order the rows of the dataframe
df <- df[ order(-df[,ncol(df)]), ]
fileNameT <- paste(output,algorithm,"/", algorithm,".csv")
#save the dataframe
write.table(df, file= fileNameT, quote = FALSE, sep="," , row.names = FALSE,
col.names = TRUE)
}
?append
debugSource('D:/workspace/IMIBIC/datasets/diabetes/v9/SearchBasedRankingFeatures.R')
outputTemp
dir.create(outputTemp)
outputTemp
dir.create("results/0h/rf/")
dir.create("results/0h/rf/", recursive = TRUE)
debugSource('D:/workspace/IMIBIC/datasets/diabetes/v9/SearchBasedRankingFeatures.R')
